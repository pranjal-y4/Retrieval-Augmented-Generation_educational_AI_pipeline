{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBLe39xHoVAL"
      },
      "source": [
        "# Introduction to the AI Prototype\n",
        "\n",
        "## Overview\n",
        "\n",
        "Welcome to the AI Research prototype notebook. This notebook is designed to demonstrate the **core AI infrastructure and workflows**, including:\n",
        "\n",
        "- **Retrieval-Augmented Generation (RAG) Systems**: Efficiently retrieving and integrating educational content across multiple subjects and curricula.\n",
        "- **Multimodal AI Systems**: Processing and combining both text and visual data for advanced educational tools.\n",
        "- **Agentic AI Frameworks**: Coordinating multiple AI agents to automate and optimize educational planning and assessment workflows.\n",
        "- **Foundation Model Integration**: Leveraging pre-trained language models for curriculum-aligned content generation and adaptive learning.\n",
        "\n",
        "This project is aligned with MESO’s mission to create the **world’s most advanced educational planning tools**.\n",
        "\n",
        "---\n",
        "\n",
        "## Purpose of this Notebook\n",
        "\n",
        "The goal of this notebook is to:\n",
        "\n",
        "1. **Provide a structured framework** for developing AI models, data pipelines, and multimodal processing workflows.\n",
        "2. **Demonstrate practical implementations** of RAG systems, embeddings, vector search, and multimodal AI.\n",
        "3. **Enable reproducible AI experiments** aligned with the technical requirements outlined in the MESO AI Researcher role.\n",
        "4. **Prepare for platform integration**, including API deployment, real-time content generation, and global scalability.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JipFlvQnmo3Y"
      },
      "source": [
        "\n",
        "## Imports\n",
        "\n",
        "# Importing Libraries and Utilities\n",
        "\n",
        "## Overview\n",
        "\n",
        "This cell loads all the essential Python libraries, utilities, and frameworks needed to **develop MESO’s AI infrastructure**, including RAG systems, multimodal AI pipelines, and data processing workflows. These imports are foundational for handling **text, embeddings, vector search, PDFs, images, and optional LLM integrations**.\n",
        "\n",
        "By organizing all imports at the beginning, we ensure:\n",
        "\n",
        "- Code readability and maintainability.\n",
        "- Easy access to essential libraries for AI, NLP, and data processing.\n",
        "- Flexibility to switch between different tools (e.g., PyMuPDF vs pdfminer) depending on availability.\n",
        "\n",
        "---\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A9Cd48y8kx4",
        "outputId": "05cbb455-f8db-4de0-9c32-55535829a457"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.2\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.12.0\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.55.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.34.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rank_bm25) (2.0.2)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (0.116.1)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.35.0)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.47.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from fastapi) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (4.14.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.48.0,>=0.40.0->fastapi) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.48.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.1.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.48.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m128.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m188.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [streamlit]\n",
            "\u001b[1A\u001b[2KSuccessfully installed pydeck-0.9.1 streamlit-1.48.1\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.19)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.23.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.34.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.1.7)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2025.8.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->timm) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (11.3.0)\n",
            "Collecting trafilatura\n",
            "  Downloading trafilatura-2.0.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from trafilatura) (2025.8.3)\n",
            "Requirement already satisfied: charset_normalizer>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from trafilatura) (3.4.3)\n",
            "Collecting courlan>=1.3.2 (from trafilatura)\n",
            "  Downloading courlan-1.3.2-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting htmldate>=1.9.2 (from trafilatura)\n",
            "  Downloading htmldate-1.9.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting justext>=3.0.1 (from trafilatura)\n",
            "  Downloading justext-3.0.2-py2.py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from trafilatura) (5.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.12/dist-packages (from trafilatura) (2.5.0)\n",
            "Requirement already satisfied: babel>=2.16.0 in /usr/local/lib/python3.12/dist-packages (from courlan>=1.3.2->trafilatura) (2.17.0)\n",
            "Collecting tld>=0.13 (from courlan>=1.3.2->trafilatura)\n",
            "  Downloading tld-0.13.1-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting dateparser>=1.1.2 (from htmldate>=1.9.2->trafilatura)\n",
            "  Downloading dateparser-1.2.2-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.9.0.post0 in /usr/local/lib/python3.12/dist-packages (from htmldate>=1.9.2->trafilatura) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.12/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2025.2)\n",
            "Requirement already satisfied: regex>=2024.9.11 in /usr/local/lib/python3.12/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2024.11.6)\n",
            "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.12/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (5.3.1)\n",
            "Collecting lxml_html_clean (from lxml[html_clean]>=4.4.2->justext>=3.0.1->trafilatura)\n",
            "  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.9.0.post0->htmldate>=1.9.2->trafilatura) (1.17.0)\n",
            "Downloading trafilatura-2.0.0-py3-none-any.whl (132 kB)\n",
            "Downloading courlan-1.3.2-py3-none-any.whl (33 kB)\n",
            "Downloading htmldate-1.9.3-py3-none-any.whl (31 kB)\n",
            "Downloading dateparser-1.2.2-py3-none-any.whl (315 kB)\n",
            "Downloading justext-3.0.2-py2.py3-none-any.whl (837 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m837.9/837.9 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tld-0.13.1-py2.py3-none-any.whl (274 kB)\n",
            "Downloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: tld, lxml_html_clean, dateparser, courlan, justext, htmldate, trafilatura\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [trafilatura]\n",
            "\u001b[1A\u001b[2KSuccessfully installed courlan-1.3.2 dateparser-1.2.2 htmldate-1.9.3 justext-3.0.2 lxml_html_clean-0.4.2 tld-0.13.1 trafilatura-2.0.0\n",
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six) (3.4.3)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m182.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m160.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF, pdfminer.six\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [pdfminer.six]\n",
            "\u001b[1A\u001b[2KSuccessfully installed PyMuPDF-1.26.3 pdfminer.six-20250506\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.11.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.1)\n",
            "Collecting git+https://github.com/salesforce/BLIP.git\n",
            "  Cloning https://github.com/salesforce/BLIP.git to /tmp/pip-req-build-e06zg3d3\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/salesforce/BLIP.git /tmp/pip-req-build-e06zg3d3\n",
            "  Resolved https://github.com/salesforce/BLIP.git to commit 3a29b7410476bf5f2ba0955827390eb6ea1f4f9d\n",
            "\u001b[31mERROR: git+https://github.com/salesforce/BLIP.git does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# =============================\n",
        "# Install Dependencies\n",
        "# =============================\n",
        "!pip install --upgrade pip\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install faiss-cpu\n",
        "!pip install sentence-transformers\n",
        "!pip install transformers\n",
        "!pip install rank_bm25\n",
        "!pip install scikit-learn\n",
        "!pip install fastapi uvicorn\n",
        "!pip install streamlit\n",
        "!pip install pillow\n",
        "!pip install opencv-python\n",
        "!pip install timm\n",
        "!pip install trafilatura\n",
        "!pip install PyMuPDF pdfminer.six\n",
        "!pip install pydantic\n",
        "\n",
        "# Optional: for multimodal (BLIP)\n",
        "!pip install git+https://github.com/salesforce/BLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULP-4Lh38roF",
        "outputId": "19a5bae1-f3df-4a9f-bb3b-2b58c869f17b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import os, re, io, json, pickle, math, random, time, warnings\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple, Optional, Any\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# sentence-transformers and cross-encoder\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "# FAISS\n",
        "import faiss\n",
        "\n",
        "# PDF libs (prefer PyMuPDF/fitz, fallback to pdfminer)\n",
        "try:\n",
        "    import fitz  # PyMuPDF\n",
        "    PDF_LIB = 'pymupdf'\n",
        "except Exception:\n",
        "    from pdfminer.high_level import extract_text as pdfminer_extract_text\n",
        "    PDF_LIB = 'pdfminer'\n",
        "\n",
        "# Text utilities\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Image libs\n",
        "from PIL import Image\n",
        "\n",
        "# Optional OpenAI\n",
        "try:\n",
        "    import openai\n",
        "except Exception:\n",
        "    openai = None\n",
        "\n",
        "# Silence warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APAqjpkkp8CK"
      },
      "source": [
        "# Cell 2 — Configuration, Persistence Paths, and Lazy Model Initialization\n",
        "\n",
        "## Overview\n",
        "\n",
        "This cell sets up the **core configuration, folder structure, and model initialization routines**. It ensures that all files, embeddings, and indexes are stored in a consistent location and that **models are only loaded when needed** (lazy loading), which saves memory and speeds up notebook execution. It also checks for **multimodal support**, enabling image-text processing if the required libraries are installed.\n",
        "\n",
        "---\n",
        "\n",
        "1. BASE_DIR: Root directory of the project.\n",
        "2. UPLOAD_DIR: Location for storing uploaded educational materials (PDFs, images, etc.).\n",
        "3. OUTPUTS: Main directory to store results, embeddings, and indexes.\n",
        "4. FAISS_DIR: Directory for storing FAISS vector indexes for semantic search.\n",
        "5. CLIP_DIR: Directory for storing CLIP embeddings (used for multimodal image-text tasks).\n",
        "6. CORPUS_PKL: Pickle file for storing metadata about the text corpus.\n",
        "\n",
        "---\n",
        "### Purpose: Load the models lazily.\n",
        "Benefits:\n",
        "- Saves RAM until model is required.\n",
        "- Avoids long initial startup times.\n",
        "- Allows switching models easily via environment variables.\n",
        "\n",
        "* load_encoder(): Loads the SentenceTransformer model for generating embeddings.\n",
        "* load_reranker(): Loads the CrossEncoder model for relevance scoring or reranking search results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fn39OXRk9mXz"
      },
      "outputs": [],
      "source": [
        "# CELL 2 — Config, persistence paths, lazy model init\n",
        "# =====================\n",
        "# Paths\n",
        "BASE_DIR = Path('.').absolute()\n",
        "UPLOAD_DIR = BASE_DIR / 'outputs' / 'uploads'\n",
        "OUTPUTS = BASE_DIR / 'outputs'\n",
        "FAISS_DIR = OUTPUTS / 'faiss'\n",
        "CLIP_DIR = OUTPUTS / 'clip_index'\n",
        "CORPUS_PKL = OUTPUTS / 'corpus_meta.pkl'\n",
        "\n",
        "'''Ensures that all necessary directories are created automatically.\n",
        "parents=True allows creation of parent directories if they don’t exist.\n",
        "exist_ok=True avoids errors if the directories already exist.'''\n",
        "for d in [UPLOAD_DIR, OUTPUTS, FAISS_DIR, CLIP_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Models\n",
        "'''ENCODER_MODEL: Default sentence transformer model used to generate embeddings for semantic search.\n",
        "RERANKER_MODEL: Cross-encoder model used to rerank candidate search results for better relevance.\n",
        "EMBED_BATCH: Number of documents processed per batch when generating embeddings; balances speed and memory usage.'''\n",
        "\n",
        "ENCODER_MODEL = os.getenv('ENCODER_MODEL', 'sentence-transformers/all-MiniLM-L6-v2')\n",
        "RERANKER_MODEL = os.getenv('RERANKER_MODEL', 'cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "EMBED_BATCH = int(os.getenv('EMBED_BATCH', '32'))\n",
        "\n",
        "# LLM settings (user should set environment variable OPENAI_API_KEY or edit below)\n",
        "''''USE_LLM: Flag to indicate whether to use OpenAI GPT models.\n",
        "OPENAI_API_KEY: Reads the API key from environment variables for secure authentication.\n",
        "If a valid key is found, it initializes the OpenAI library for downstream content generation.'''\n",
        "USE_LLM = False\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', None)\n",
        "if OPENAI_API_KEY:\n",
        "    openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "# Lazy-loaded model holders\n",
        "'''_encoder and _reranker: Placeholders for the embedding and cross-encoder models.\n",
        "These are initialized only when the load_encoder() or load_reranker() functions are called, saving memory until the models are needed.'''\n",
        "\n",
        "_encoder: Optional[SentenceTransformer] = None\n",
        "_reranker: Optional[CrossEncoder] = None\n",
        "\n",
        "\n",
        "def load_encoder():\n",
        "    global _encoder\n",
        "    if _encoder is None:\n",
        "        print('Loading encoder:', ENCODER_MODEL)\n",
        "        _encoder = SentenceTransformer(ENCODER_MODEL)\n",
        "    return _encoder\n",
        "\n",
        "\n",
        "def load_reranker():\n",
        "    global _reranker\n",
        "    if _reranker is None:\n",
        "        print('Loading reranker:', RERANKER_MODEL)\n",
        "        _reranker = CrossEncoder(RERANKER_MODEL)\n",
        "    return _reranker\n",
        "\n",
        "# Multimodal flags (will be lazy-loaded if used)\n",
        "'''Purpose: Checks if multimodal libraries are installed.\n",
        "CLIP / BLIP: Used for vision-language tasks, such as matching images to text or generating visual content embeddings.\n",
        "MULTIMODAL_AVAILABLE: Boolean flag indicating whether image-text processing is supported. Allows the notebook to skip multimodal operations if dependencies are missing.'''\n",
        "\n",
        "MULTIMODAL_AVAILABLE = False\n",
        "try:\n",
        "    from transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration\n",
        "    import torch\n",
        "    MULTIMODAL_AVAILABLE = True\n",
        "except Exception:\n",
        "    MULTIMODAL_AVAILABLE = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqqX7196rou-"
      },
      "source": [
        "## Upload helpers (Colab / Jupyter / Streamlit)\n",
        "---\n",
        "## The functions below provide three upload interfaces. Each writes to outputs/uploads/ and returns paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bODyKKZN9zei"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# 1) Colab upload helper (if running in Colab)\n",
        "def colab_upload_handler():\n",
        "    try:\n",
        "        from google.colab import files\n",
        "    except Exception:\n",
        "        print('Colab environment not detected.')\n",
        "        return []\n",
        "    uploaded = files.upload()\n",
        "    paths=[]\n",
        "    for name, blob in uploaded.items():\n",
        "        p = UPLOAD_DIR / name\n",
        "        with open(p, 'wb') as f:\n",
        "            f.write(blob)\n",
        "        paths.append(str(p))\n",
        "    print('Uploaded:', paths)\n",
        "    return paths\n",
        "\n",
        "# 2) Jupyter ipywidgets (if running in Jupyter)\n",
        "def jupyter_file_upload_widget():\n",
        "    try:\n",
        "        import ipywidgets as widgets\n",
        "        from IPython.display import display\n",
        "    except Exception:\n",
        "        print('ipywidgets not available in this environment.')\n",
        "        return None\n",
        "    upload = widgets.FileUpload(accept='.pdf', multiple=True)\n",
        "    display(upload)\n",
        "    print('Use the widget to upload. After uploading, run: handle_widget_upload(upload)')\n",
        "    return upload\n",
        "\n",
        "\n",
        "def handle_widget_upload(widget):\n",
        "    paths=[]\n",
        "    for name, file_info in widget.value.items():\n",
        "        p = UPLOAD_DIR / name\n",
        "        with open(p, 'wb') as f:\n",
        "            f.write(file_info['content'])\n",
        "        paths.append(str(p))\n",
        "    print('Saved uploaded files:', paths)\n",
        "    return paths\n",
        "\n",
        "# 3) Streamlit uploader (for demo)\n",
        "# In streamlit app: uploaded_files = st.file_uploader('Upload PDFs', type=['pdf'], accept_multiple_files=True)\n",
        "# Then call streamlit_handle_upload(uploaded_files)\n",
        "def streamlit_handle_upload(uploaded_files):\n",
        "    paths=[]\n",
        "    for f in uploaded_files:\n",
        "        # f is a UploadedFile-like object\n",
        "        p = UPLOAD_DIR / f.name\n",
        "        with open(p, 'wb') as out:\n",
        "            out.write(f.getbuffer())\n",
        "        paths.append(str(p))\n",
        "    return paths\n",
        "\n",
        "# Unified helper: call with a list of file-like or paths -> copy into outputs/uploads and return list\n",
        "\n",
        "def ingest_files(filepaths: List[str]) -> List[str]:\n",
        "    \"\"\"Copy given file paths (e.g. user-selected) to UPLOAD_DIR and return new paths.\"\"\"\n",
        "    saved=[]\n",
        "    for fp in filepaths:\n",
        "        src = Path(fp)\n",
        "        if not src.exists():\n",
        "            print('[WARN] source does not exist:', fp)\n",
        "            continue\n",
        "        dst = UPLOAD_DIR / src.name\n",
        "        with open(src,'rb') as r, open(dst,'wb') as w:\n",
        "            w.write(r.read())\n",
        "        saved.append(str(dst))\n",
        "    print('Ingested files ->', saved)\n",
        "    return saved\n",
        "\n",
        "# Notify UX helper\n",
        "\n",
        "def notify_upload_success(paths: List[str]):\n",
        "    if not paths:\n",
        "        print('No files uploaded.')\n",
        "        return\n",
        "    print('Successfully uploaded and stored the following files:')\n",
        "    for p in paths:\n",
        "        print(' -', p)\n",
        "    print('\\nNext: building corpus from these PDFs (call build_corpus_from_folder)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HsuQAros1yY"
      },
      "source": [
        "# Robust PDF Extraction (Handles Images Too)\n",
        "\n",
        "## Overview\n",
        "\n",
        "This cell defines **robust functions for extracting text and images from PDFs**. It is designed to:\n",
        "\n",
        "1. Handle **both text and images** in PDF pages.\n",
        "2. Support **two different PDF libraries** (`PyMuPDF` and `pdfminer`) to maximize compatibility.\n",
        "3. Allow **batch processing of PDFs in a folder**, generating structured output for downstream RAG and multimodal AI workflows.\n",
        "\n",
        "These functions are essential for MESO’s AI system because educational materials often come in PDF format and may include diagrams, charts, or scanned pages.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Function: `extract_text_and_images_from_pdf`\n",
        "\n",
        "```python\n",
        "def extract_text_and_images_from_pdf(pdf_path: str, extract_images: bool=True) -> Dict[str, Any]:\n",
        "    \"\"\"Return {'pages': [{'page':n,'text':str,'images':[{'name','data','ext'}]}], 'meta': {...}}\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGL1aqWX-BYC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1. Function: `extract_text_and_images_from_pdf`\n",
        "'''Reads a single PDF file.\n",
        "Extracts all text from each page.\n",
        "Optionally extracts embedded images.\n",
        "Returns a dictionary containing:\n",
        "pages: List of pages, each with page number, text, and images.\n",
        "meta: Metadata about the PDF (e.g., file path).'''\n",
        "\n",
        "def extract_text_and_images_from_pdf(pdf_path: str, extract_images: bool=True) -> Dict[str, Any]:\n",
        "    \"\"\"Return {'pages': [{'page':n,'text':str,'images':[{'name','data','ext'}]}], 'meta': {...}}\"\"\"\n",
        "    pdf_path = str(pdf_path)\n",
        "    out = {'pages':[], 'meta':{'path':pdf_path}}\n",
        "    if PDF_LIB == 'pymupdf':\n",
        "        doc = fitz.open(pdf_path)\n",
        "        for i in range(len(doc)):\n",
        "            page = doc[i]\n",
        "            text = page.get_text('text') or ''\n",
        "            images = []\n",
        "            if extract_images:\n",
        "                for img_index, img in enumerate(page.get_images(full=True)):\n",
        "                    xref = img[0]\n",
        "                    base_image = doc.extract_image(xref)\n",
        "                    img_bytes = base_image['image']\n",
        "                    ext = base_image.get('ext','png')\n",
        "                    name = f\"{Path(pdf_path).stem}_p{i+1}_img{img_index}.{ext}\"\n",
        "                    images.append({'name':name, 'data':img_bytes, 'ext':ext})\n",
        "            out['pages'].append({'page': i+1, 'text': text, 'images': images})\n",
        "        doc.close()\n",
        "    else:\n",
        "        # pdfminer fallback: extract text only\n",
        "        text = pdfminer_extract_text(pdf_path)\n",
        "        # basic split into pages by form feed if present\n",
        "        pages = text.split('\\x0c')\n",
        "        for i, ptxt in enumerate(pages):\n",
        "            out['pages'].append({'page': i+1, 'text': ptxt, 'images': []})\n",
        "    return out\n",
        "\n",
        "# Robust wrapper for a folder of PDFs\n",
        "# 2. Function: extract_from_folder\n",
        "'''Processes all PDFs in a folder.\n",
        "Returns two lists:\n",
        "corpus_pages: All pages and extracted content.\n",
        "pdf_metas: Metadata for each PDF (path, number of pages).'''\n",
        "\n",
        "def extract_from_folder(folder: str, extract_images: bool=True) -> Tuple[List[Dict], List[Dict]]:\n",
        "    folder = Path(folder)\n",
        "    pdfs = sorted([p for p in folder.glob('*.pdf')])\n",
        "    corpus_pages=[]\n",
        "    pdf_metas=[]\n",
        "    for pdf in pdfs:\n",
        "        try:\n",
        "            res = extract_text_and_images_from_pdf(str(pdf), extract_images=extract_images)\n",
        "            corpus_pages.append({'pdf': str(pdf), 'pages': res['pages']})\n",
        "            pdf_metas.append({'path': str(pdf), 'n_pages': len(res['pages'])})\n",
        "        except Exception as e:\n",
        "            print('[ERROR] Failed to process', pdf, e)\n",
        "    return corpus_pages, pdf_metas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3xpIEnnuAv9"
      },
      "source": [
        "# Header/Footer Detection & Cleaning\n",
        "\n",
        "## Overview\n",
        "\n",
        "This cell defines **functions to automatically detect and remove repeating headers and footers** from PDFs.  \n",
        "\n",
        "In educational PDFs, it is common for each page to contain:\n",
        "\n",
        "- Document titles, chapter names, or school names at the **top** (headers)\n",
        "- Page numbers, footnotes, or copyright notes at the **bottom** (footers)\n",
        "\n",
        "These repeated elements can interfere with **text embeddings, semantic search, and RAG systems**, so cleaning them is essential.\n",
        "\n",
        "The two main functions are:\n",
        "\n",
        "1. `detect_repeating_headers_footers` — identifies lines that are repeated across pages and are likely headers or footers.\n",
        "2. `remove_repeating_headers` — removes these detected lines from page text to produce cleaner, content-focused text.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1-bHLnt-GQR"
      },
      "outputs": [],
      "source": [
        "# CELL 5 — Header/footer detection & cleaning\n",
        "# =====================\n",
        "\n",
        "def detect_repeating_headers_footers(pages: List[Dict], top_lines=4, bottom_lines=4, threshold=0.25) -> set:\n",
        "    \"\"\"Given pages [{'text':...}], return set of repeated line fragments likely headers/footers\"\"\"\n",
        "    total = len(pages)\n",
        "    c = Counter()\n",
        "    for p in pages:\n",
        "        lines = [ln.strip() for ln in p['text'].splitlines() if ln.strip()]\n",
        "        top = lines[:top_lines]\n",
        "        bottom = lines[-bottom_lines:]\n",
        "        for ln in top+bottom:\n",
        "            norm = re.sub(r'\\d+','', ln.lower()).strip()\n",
        "            if len(norm) > 3:\n",
        "                c[norm]+=1\n",
        "    candidates = {k for k,v in c.items() if v/total >= threshold}\n",
        "    return candidates\n",
        "\n",
        "'''Removes all previously detected repeating headers/footers from a given page’s text.\n",
        "'''\n",
        "def remove_repeating_headers(text: str, repeating: set) -> str:\n",
        "    out = text\n",
        "    for h in repeating:\n",
        "        try:\n",
        "            out = re.sub(re.escape(h), ' ', out, flags=re.I)\n",
        "        except Exception:\n",
        "            continue\n",
        "    out = re.sub(r'\\s+', ' ', out).strip()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go8L9vShu5d3"
      },
      "source": [
        "# Semantic Chunking (Heading Detection + Sentence-Aware Overlap)\n",
        "\n",
        "## Overview\n",
        "\n",
        "This cell defines **functions to split large PDF/text content into semantically meaningful chunks** for embedding and retrieval.  \n",
        "\n",
        "Key goals:\n",
        "\n",
        "1. Detect **logical headings** or sections in the text.\n",
        "2. Preserve **sentence boundaries** to maintain context.\n",
        "3. Introduce **overlap between chunks** to avoid breaking semantic continuity.\n",
        "4. Produce chunks of **manageable size** for vector embedding models (like SentenceTransformers or LLMs).\n",
        "\n",
        "This is critical for MESO’s AI system because educational content often comes as **long PDFs**, and splitting them properly ensures:\n",
        "\n",
        "- High-quality embeddings.\n",
        "- Improved retrieval accuracy in RAG systems.\n",
        "- Preservation of contextual information across chunk boundaries.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qm08EXc-LRW"
      },
      "outputs": [],
      "source": [
        "# CELL 6 — Semantic chunking (heading detection + sentence-aware overlap)\n",
        "# =====================\n",
        "\n",
        "def heuristic_heading_lines(text: str, top_n=6) -> List[str]:\n",
        "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
        "    cand = []\n",
        "    top = lines[:top_n]\n",
        "    for ln in top:\n",
        "        # heading heuristics: ALL CAPS or short line (<=8 words) or contains 'Chapter' 'Section'\n",
        "        if ln.isupper() or len(ln.split()) <= 8 or re.search(r'chapter|section|unit|lesson', ln, flags=re.I):\n",
        "            cand.append(ln)\n",
        "    return cand\n",
        "\n",
        "\n",
        "def chunk_by_section_and_sentences(text: str, chunk_words=500, overlap_words=100, min_words=40) -> List[str]:\n",
        "    # Split text into sections by two newlines or headings\n",
        "    # keep sentence boundaries\n",
        "    sections = re.split(r'\\n\\s*\\n', text)\n",
        "    chunks=[]\n",
        "    for sec in sections:\n",
        "        sec = sec.strip()\n",
        "        if not sec: continue\n",
        "        sents = sent_tokenize(sec)\n",
        "        if not sents: continue\n",
        "        bucket=[]\n",
        "        bucket_words=0\n",
        "        for s in sents:\n",
        "            w = len(s.split())\n",
        "            if bucket_words + w <= chunk_words or not bucket:\n",
        "                bucket.append(s); bucket_words += w\n",
        "            else:\n",
        "                chunk = ' '.join(bucket).strip()\n",
        "                if len(chunk.split()) >= min_words:\n",
        "                    chunks.append(chunk)\n",
        "                # compute overlap in sentences to keep semantic continuity\n",
        "                avg = max(1, bucket_words/len(bucket))\n",
        "                overlap_sent = max(1, int(overlap_words/avg))\n",
        "                bucket = bucket[-overlap_sent:].copy()\n",
        "                bucket_words = sum(len(ss.split()) for ss in bucket)\n",
        "                bucket.append(s); bucket_words += w\n",
        "        if bucket:\n",
        "            chunk = ' '.join(bucket).strip()\n",
        "            if len(chunk.split()) >= min_words:\n",
        "                chunks.append(chunk)\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_Dlf-vEve5o"
      },
      "source": [
        "# Build Corpus from Uploaded PDFs and Persist\n",
        "\n",
        "## Overview\n",
        "\n",
        "This cell defines a **function to process uploaded PDFs into a clean, chunked, and structured corpus**, ready for embeddings and retrieval.  \n",
        "\n",
        "Key responsibilities of this cell:\n",
        "\n",
        "1. **Load uploaded PDFs** from a folder.\n",
        "2. **Extract text** (and optionally images) from PDFs.\n",
        "3. **Detect and remove repeating headers/footers** for clean content.\n",
        "4. **Chunk text semantically** using sentence-aware overlap.\n",
        "5. **Fallback sliding window** for unchunked text to avoid losing data.\n",
        "6. **Generate metadata** for each chunk for tracking.\n",
        "7. **Persist the corpus** to disk for future use.\n",
        "\n",
        "This step **consolidates PDF preprocessing, cleaning, and chunking** into a reusable corpus for MESO’s AI RAG system.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7Vo1WMl-P-6"
      },
      "outputs": [],
      "source": [
        "# CELL 7 — Build corpus from uploaded PDFs and persist\n",
        "# =====================\n",
        "\n",
        "def build_corpus_from_uploads(upload_folder: str, persist=True) -> Tuple[List[str], List[Dict]]:\n",
        "    \"\"\"Extracts, cleans headers, chunks, and returns corpus list and meta list. Persists to CORPUS_PKL by default.\"\"\"\n",
        "    upload_folder = Path(upload_folder)\n",
        "    # extract pages + images\n",
        "    pdfs = sorted([p for p in upload_folder.glob('*.pdf')])\n",
        "    all_pages=[]\n",
        "    for pdf in pdfs:\n",
        "        try:\n",
        "            doc = fitz.open(str(pdf)) if PDF_LIB=='pymupdf' else None\n",
        "        except Exception:\n",
        "            doc = None\n",
        "        # use extract_from_folder helper for robustness\n",
        "        pages_struct, pm = extract_from_folder(str(upload_folder), extract_images=False)\n",
        "        # pages_struct contains entries for all pdfs in folder. We'll aggregate differently below\n",
        "        break\n",
        "    # Simpler: individually process each pdf to keep mapping\n",
        "    corpus=[]\n",
        "    meta=[]\n",
        "    for pdf in pdfs:\n",
        "        try:\n",
        "            res = extract_text_and_images_from_pdf(str(pdf), extract_images=False)\n",
        "        except Exception as e:\n",
        "            print('[ERROR]', e)\n",
        "            continue\n",
        "        pages = res['pages']\n",
        "        # detect headers across pages for this pdf\n",
        "        repeating = detect_repeating_headers_footers(pages, top_lines=4, bottom_lines=4, threshold=0.25)\n",
        "        for p in pages:\n",
        "            txt = p['text']\n",
        "            if not txt or not txt.strip():\n",
        "                continue\n",
        "            txt = remove_repeating_headers(txt, repeating)\n",
        "            chunks = chunk_by_section_and_sentences(txt, chunk_words=500, overlap_words=100, min_words=40)\n",
        "            if not chunks:\n",
        "                # fallback sliding window\n",
        "                words = txt.split()\n",
        "                if len(words) > 60:\n",
        "                    for i in range(0, len(words), 300):\n",
        "                        sub = ' '.join(words[i:i+500])\n",
        "                        corpus.append(sub)\n",
        "                        meta.append({'doc': str(pdf.name), 'page': p['page'], 'chunk_id': len(corpus)-1})\n",
        "                else:\n",
        "                    corpus.append(txt)\n",
        "                    meta.append({'doc': str(pdf.name), 'page': p['page'], 'chunk_id': len(corpus)-1})\n",
        "                continue\n",
        "            for cid, ch in enumerate(chunks):\n",
        "                corpus.append(ch)\n",
        "                mm = {'doc': str(pdf.name), 'page': p['page'], 'chunk_id': cid, 'n_words': len(ch.split()), 'first_sentence': ch.split('. ')[0][:200]}\n",
        "                meta.append(mm)\n",
        "    print('Built corpus chunks:', len(corpus))\n",
        "    if persist:\n",
        "        with open(CORPUS_PKL, 'wb') as f:\n",
        "            pickle.dump({'corpus':corpus,'meta':meta}, f)\n",
        "        print('Persisted corpus ->', CORPUS_PKL)\n",
        "    return corpus, meta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlNrVh-ivtBR"
      },
      "source": [
        "# Cell 8 — Build Embeddings + FAISS Index (Batched)\n",
        "\n",
        "## Overview\n",
        "\n",
        "This cell defines functions to **convert the processed corpus into embeddings and build a FAISS index**, enabling **fast semantic search**.  \n",
        "\n",
        "Key responsibilities:\n",
        "\n",
        "1. Encode each text chunk in the corpus into **vector embeddings**.\n",
        "2. Build a **FAISS index** for efficient similarity search.\n",
        "3. Persist embeddings, index, and metadata to disk.\n",
        "4. Provide a loader function to **reload the FAISS index and metadata**.\n",
        "\n",
        "This is a critical step for MESO’s AI retrieval system, allowing **rapid, accurate retrieval of educational content** from large document corpora.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ogIR-Yv-Tmo"
      },
      "outputs": [],
      "source": [
        "# CELL 8 — Build embeddings + FAISS index (batched)\n",
        "# =====================\n",
        "\n",
        "def build_faiss_index(corpus: List[str], meta: List[Dict], encoder_model=None, batch_size=EMBED_BATCH, persist_dir: Path=FAISS_DIR):\n",
        "    if encoder_model is None:\n",
        "        encoder_model = load_encoder()\n",
        "    all_emb=[]\n",
        "    for i in tqdm(range(0, len(corpus), batch_size), desc='Embedding corpus'):\n",
        "        batch = corpus[i:i+batch_size]\n",
        "        emb = encoder_model.encode(batch, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)\n",
        "        all_emb.append(emb)\n",
        "    emb = np.vstack(all_emb).astype('float32')\n",
        "    dim = emb.shape[1]\n",
        "    index = faiss.IndexIDMap(faiss.IndexFlatIP(dim))\n",
        "    ids = np.arange(len(corpus)).astype('int64')\n",
        "    index.add_with_ids(emb, ids)\n",
        "    # persist\n",
        "    faiss.write_index(index, str(persist_dir / 'faiss.index'))\n",
        "    np.save(str(persist_dir / 'embeddings.npy'), emb)\n",
        "    with open(str(persist_dir / 'meta.pkl'), 'wb') as f:\n",
        "        pickle.dump(meta, f)\n",
        "    print('Saved FAISS index to', persist_dir)\n",
        "    return index, emb\n",
        "\n",
        "# Loader\n",
        "\n",
        "def load_index_and_meta(persist_dir: Path=FAISS_DIR):\n",
        "    idx_path = persist_dir / 'faiss.index'\n",
        "    if not idx_path.exists():\n",
        "        raise FileNotFoundError('No FAISS index found. Run build_faiss_index first.')\n",
        "    index = faiss.read_index(str(idx_path))\n",
        "    emb = np.load(str(persist_dir / 'embeddings.npy'))\n",
        "    with open(str(persist_dir / 'meta.pkl'), 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "    print('Loaded FAISS index and meta. n_chunks=', len(meta))\n",
        "    return index, emb, meta\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nITT-BJbwcgd"
      },
      "source": [
        "# Search + Rerank + Filters + Confidence\n",
        "\n",
        "## Overview\n",
        "\n",
        "This cell defines functions to **search the FAISS index, rerank retrieved chunks using a cross-encoder, apply optional filters, and compute confidence scores**.  \n",
        "\n",
        "It is a **core retrieval step** in the MESO AI pipeline, allowing semantic search over educational content with **high precision and contextual relevance**.\n",
        "\n",
        "The two main functions are:\n",
        "\n",
        "1. `search_and_rerank` — retrieve and rerank top chunks from the corpus.\n",
        "2. `safe_confidence` — compute a normalized confidence score for retrieved results.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rekT5DZ-W8v"
      },
      "outputs": [],
      "source": [
        "# CELL 9 — Search + rerank + filters + confidence\n",
        "# =====================\n",
        "\n",
        "def search_and_rerank(index, query: str, corpus: List[str], meta: List[Dict], top_k=5, extra_k=12, filters: Optional[Dict]=None):\n",
        "    encoder = load_encoder()\n",
        "    reranker = load_reranker()\n",
        "    qv = encoder.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
        "    D, I = index.search(qv, extra_k)\n",
        "    cand = []\n",
        "    for score, idx in zip(D[0], I[0]):\n",
        "        if int(idx) < 0: continue\n",
        "        m = meta[int(idx)]\n",
        "        if filters:\n",
        "            skip=False\n",
        "            for k,v in filters.items():\n",
        "                mv = m.get(k)\n",
        "                if mv is None or str(mv).lower() != str(v).lower():\n",
        "                    skip=True; break\n",
        "            if skip: continue\n",
        "        cand.append((int(idx), float(score)))\n",
        "    if not cand:\n",
        "        return [], [], []\n",
        "    chunks = [corpus[i] for i,_ in cand]\n",
        "    pairs = [(query, c) for c in chunks]\n",
        "    rerank_scores = reranker.predict(pairs)\n",
        "    ranked = sorted(zip(cand, chunks, rerank_scores), key=lambda x: x[2], reverse=True)[:top_k]\n",
        "    final_chunks = [x[1] for x in ranked]\n",
        "    final_meta = [meta[x[0][0]] for x in ranked]\n",
        "    final_scores = [float(x[2]) for x in ranked]\n",
        "    return final_chunks, final_meta, final_scores\n",
        "\n",
        "\n",
        "def safe_confidence(scores: List[float]) -> float:\n",
        "    if not scores: return 0.0\n",
        "    return float(np.clip(np.mean(scores), 0.0, 1.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WOo1XFSwtG_"
      },
      "source": [
        "# Guardrails & Validator Agent\n",
        "\n",
        "## Overview\n",
        "\n",
        "This cell introduces a **validator agent**, which acts as a **content guardrail** in the MESO AI pipeline.  \n",
        "\n",
        "The purpose of this function is to **filter retrieved or generated text chunks**, ensuring they:\n",
        "\n",
        "1. Meet **minimum content length**.\n",
        "2. Avoid **banned or harmful terms**.\n",
        "3. Optionally respect **readability constraints** (e.g., Flesch-Kincaid grade level).\n",
        "\n",
        "This is crucial for **educational applications**, where outputs must be safe, clean, and appropriate for learners.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8mnwqgS-ZoG"
      },
      "outputs": [],
      "source": [
        "# CELL 10 — Guardrails & validator agent\n",
        "# =====================\n",
        "\n",
        "def validator_agent(chunks: List[str], metas: List[Dict], min_words=20, banned_terms: Optional[set]=None, max_readability_grade: Optional[float]=None) -> Tuple[List[str], List[Dict]]:\n",
        "    if banned_terms is None:\n",
        "        banned_terms = {'suicide','bomb','explosive','terror','hate'}\n",
        "    filtered_chunks=[]\n",
        "    filtered_meta=[]\n",
        "    for ch,m in zip(chunks, metas):\n",
        "        if len(ch.split()) < min_words:\n",
        "            continue\n",
        "        low = ch.lower()\n",
        "        if any(b in low for b in banned_terms):\n",
        "            continue\n",
        "        filtered_chunks.append(ch)\n",
        "        filtered_meta.append(m)\n",
        "    return filtered_chunks, filtered_meta\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mYGn_EkxLkL"
      },
      "source": [
        "# MCQ Generation (Deterministic + LLM-based)\n",
        "\n",
        "## Overview\n",
        "\n",
        "This cell implements the **Multiple Choice Question (MCQ) generation pipeline** for educational content in MESO.  \n",
        "It combines:\n",
        "\n",
        "1. **Deterministic MCQ generation** from extracted text chunks.\n",
        "2. **LLM-based MCQ generation** (e.g., OpenAI ChatCompletion or Google Gemini) for more natural, diverse, and pedagogically rich questions.\n",
        "3. A **wrapper function** to seamlessly choose between deterministic or LLM-based generation.\n",
        "\n",
        "The goal is to automatically generate **contextually accurate MCQs** suitable for teachers and students.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRG2M_Gs-cGv"
      },
      "outputs": [],
      "source": [
        "# CELL 11 — MCQ generation (deterministic fallback + LLM wrapper)\n",
        "# =====================\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "def deterministic_mcqs_from_chunks(chunks: List[str], n_q=5):\n",
        "    text = ' '.join(chunks)\n",
        "    vect = TfidfVectorizer(stop_words='english', max_features=2000)\n",
        "    X = vect.fit_transform([text])\n",
        "    terms = list(vect.get_feature_names_out())\n",
        "    candidates = [t for t in terms if len(t)>4]\n",
        "    sents = re.split(r'(?<=[.?!])\\s+', text)\n",
        "    random.shuffle(candidates)\n",
        "    questions=[]\n",
        "    used_terms=set()\n",
        "    for term in candidates:\n",
        "        if len(questions)>=n_q: break\n",
        "        if term in used_terms: continue\n",
        "        found = [s for s in sents if re.search(r'\\b'+re.escape(term)+r'\\b', s, flags=re.I)]\n",
        "        if not found: continue\n",
        "        sent = found[0]\n",
        "        blank = re.sub(r'(?i)\\b'+re.escape(term)+r'\\b', '_____', sent, count=1)\n",
        "        # distractors: pick other candidate terms or generate via simple perturbation\n",
        "        distractors = [c for c in candidates if c!=term]\n",
        "        if len(distractors) >= 3:\n",
        "            choices = random.sample(distractors, 3) + [term]\n",
        "        else:\n",
        "            # simple morphological variants\n",
        "            choices = [term+'s', term.upper(), term+'_alt'][:3] + [term]\n",
        "        random.shuffle(choices)\n",
        "        questions.append({'question': blank, 'options': choices, 'answer': term, 'rationale': 'Taken from source sentence.', 'difficulty': 'medium'})\n",
        "        used_terms.add(term)\n",
        "    return questions\n",
        "\n",
        "# LLM-based generator (OpenAI ChatCompletion example - user must set OPENAI_API_KEY)\n",
        "LLM_PROMPT_TEMPLATE = '''You are an educational content generator. Based only on the CONTEXT below, generate {n_q} multiple-choice questions (A-D).\n",
        "For each question include JSON keys: question_text, options (A-D), correct_option (A/B/C/D), rationale (one brief sentence), difficulty (easy/medium/hard).\n",
        "Context:\n",
        "{context}\n",
        "Respond ONLY with a JSON array.'''\n",
        "\n",
        "\n",
        "# LLM-based MCQ generator\n",
        "import requests\n",
        "\n",
        "def llm_generate_mcqs_gemini(chunks: List[str], n_q=5, api_key: Optional[str]=None, model_name='gemini-1.5'):\n",
        "    \"\"\"\n",
        "    Generate MCQs using Gemini LLM instead of OpenAI.\n",
        "    \"\"\"\n",
        "    # Get API key from argument or environment variable\n",
        "    key = api_key or os.getenv(\"GEMINI_API_KEY\")\n",
        "    if not key:\n",
        "        print(\"[WARN] No Gemini API key found. Falling back to deterministic MCQs.\")\n",
        "        return deterministic_mcqs_from_chunks(chunks, n_q=n_q)\n",
        "\n",
        "    prompt = LLM_PROMPT_TEMPLATE.format(n_q=n_q, context='\\n\\n'.join(chunks[:4]))\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    data = {\n",
        "        \"model\": model_name,\n",
        "        \"prompt\": prompt,\n",
        "        \"temperature\": 0.0,\n",
        "        \"max_output_tokens\": 800\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        resp = requests.post(\"https://api.generativeai.google/v1beta2/models/{model_name}:generateText\".format(model_name=model_name),\n",
        "                             headers=headers, json=data)\n",
        "        resp.raise_for_status()\n",
        "        txt = resp.json()['candidates'][0]['content']\n",
        "        return json.loads(txt)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Gemini LLM generation failed: {e}. Using deterministic fallback.\")\n",
        "        return deterministic_mcqs_from_chunks(chunks, n_q=n_q)\n",
        "\n",
        "\n",
        "\n",
        "def generate_mcqs(chunks: List[str], n_q=5, use_llm=False, api_key: Optional[str]=None):\n",
        "    if not chunks: return []\n",
        "    if use_llm:\n",
        "        return llm_generate_mcqs(chunks, n_q=n_q, api_key=api_key)\n",
        "    else:\n",
        "        return deterministic_mcqs_from_chunks(chunks, n_q=n_q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLKhPNZWxVMD"
      },
      "source": [
        "# Multimodal: CLIP Indexing + BLIP Captioning\n",
        "\n",
        "## Overview\n",
        "\n",
        "This cell adds **multimodal support** to the MESO pipeline, enabling the system to handle **images** alongside text.  \n",
        "It combines:\n",
        "\n",
        "1. **CLIP embeddings** for image semantic indexing and similarity search.\n",
        "2. **BLIP captions** to generate textual descriptions of images, which can then be searched or linked with text chunks.\n",
        "3. **FAISS indexing** to store and retrieve image embeddings efficiently.\n",
        "\n",
        "> Multimodal functionality is **optional** and only works if the required libraries (`transformers`, `torch`, `PIL`) are installed.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qumaHl8FAUFm"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import numpy as np\n",
        "import pickle\n",
        "import faiss\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Make sure MULTIMODAL_AVAILABLE is True before running this\n",
        "def multimodal_index_images(image_folder: str, persist_dir: Path):\n",
        "    if not MULTIMODAL_AVAILABLE:\n",
        "        print('Multimodal libs not available. Skipping image indexing.')\n",
        "        return None\n",
        "\n",
        "    # Load models\n",
        "    clip_model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\n",
        "    clip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
        "    blip_model = BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\n",
        "    blip_processor = BlipProcessor.from_pretrained('Salesforce/blip-image-captioning-base')\n",
        "\n",
        "    # Only pick valid image file types\n",
        "    valid_exts = ['.png', '.jpg', '.jpeg', '.bmp', '.tiff']\n",
        "    img_paths = sorted([str(p) for p in Path(image_folder).glob('*') if p.suffix.lower() in valid_exts])\n",
        "\n",
        "    if not img_paths:\n",
        "        print('No valid images found in', image_folder)\n",
        "        return None\n",
        "\n",
        "    emb_list = []\n",
        "    meta = []\n",
        "\n",
        "    for p in tqdm(img_paths, desc='Indexing images'):\n",
        "        try:\n",
        "            im = Image.open(p).convert('RGB')\n",
        "        except UnidentifiedImageError:\n",
        "            print(f\"⚠️ Skipping non-image file: {p}\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Skipping file {p} due to error: {e}\")\n",
        "            continue\n",
        "\n",
        "        # CLIP embedding\n",
        "        inputs = clip_processor(images=im, return_tensors='pt')\n",
        "        with torch.no_grad():\n",
        "            out = clip_model.get_image_features(**inputs)\n",
        "            vec = out.cpu().numpy().reshape(-1)\n",
        "            vec = vec / np.linalg.norm(vec)  # normalize\n",
        "        emb_list.append(vec.astype('float32'))\n",
        "\n",
        "        # BLIP captioning\n",
        "        blip_in = blip_processor(images=im, return_tensors='pt').to(blip_model.device)\n",
        "        with torch.no_grad():\n",
        "            ids = blip_model.generate(**blip_in, max_new_tokens=40)\n",
        "            caption = blip_processor.decode(ids[0], skip_special_tokens=True)\n",
        "        meta.append({'path': p, 'caption': caption})\n",
        "\n",
        "    # Save embeddings with FAISS\n",
        "    if emb_list:\n",
        "        emb = np.vstack(emb_list)\n",
        "        dim = emb.shape[1]\n",
        "        img_index = faiss.IndexIDMap(faiss.IndexFlatIP(dim))\n",
        "        ids = np.arange(len(emb)).astype('int64')\n",
        "        img_index.add_with_ids(emb, ids)\n",
        "        faiss.write_index(img_index, str(persist_dir / 'img.index'))\n",
        "        np.save(str(persist_dir / 'img_emb.npy'), emb)\n",
        "        with open(str(persist_dir / 'img_meta.pkl'), 'wb') as f:\n",
        "            pickle.dump(meta, f)\n",
        "        print(f\"✅ Saved image index with {len(emb_list)} images to {persist_dir}\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"⚠️ No valid images were indexed.\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xidb5K9GxscO"
      },
      "source": [
        "# Orchestration: Retriever → Validator → Quiz Generator → Finalizer\n",
        "\n",
        "## Overview\n",
        "\n",
        "This cell represents the **pipeline orchestration layer** of the MESO system.  \n",
        "It coordinates all previous components — text/image retrieval, validation, MCQ generation, and packaging — into a **single workflow** that transforms a user query into a ready-to-use quiz payload.\n",
        "\n",
        "The main functions are:\n",
        "\n",
        "1. `finalizer_agent`: Combines MCQs, metadata, and optionally image data into a structured output.\n",
        "2. `run_pipeline`: End-to-end orchestrator that:\n",
        "   - Searches and reranks relevant chunks\n",
        "   - Applies validator guardrails\n",
        "   - Generates MCQs (deterministic or LLM-based)\n",
        "   - Returns a final payload.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkQjST6YAfKV"
      },
      "outputs": [],
      "source": [
        "# CELL 13 — Orchestration: Retriever -> Validator -> QuizGen -> Finalizer\n",
        "# =====================\n",
        "\n",
        "def finalizer_agent(mcqs: List[Dict], metas: List[Dict], images: Optional[List[Dict]]=None) -> Dict:\n",
        "    payload = {'quiz': mcqs, 'sources': metas}\n",
        "    if images:\n",
        "        payload['images'] = images\n",
        "    payload['generated_at'] = time.strftime('%Y-%m-%d %H:%M:%S')\n",
        "    return payload\n",
        "\n",
        "\n",
        "def run_pipeline(query: str, index=None, corpus=None, meta=None, top_k=5, use_llm=False, api_key: Optional[str]=None, filters: Optional[Dict]=None):\n",
        "    # Load index & corpus if not provided\n",
        "    if index is None or corpus is None or meta is None:\n",
        "        index, emb, meta = load_index_and_meta()\n",
        "        with open(CORPUS_PKL, 'rb') as f:\n",
        "            d = pickle.load(f)\n",
        "            corpus = d['corpus']\n",
        "            meta = d['meta']\n",
        "\n",
        "    # Search and rerank\n",
        "    chunks, metas, scores = search_and_rerank(index, query, corpus, meta, top_k=top_k, extra_k=top_k*3, filters=filters)\n",
        "\n",
        "    if not chunks:\n",
        "        print('No chunks retrieved.')\n",
        "        return {'error':'no_retrieval'}\n",
        "\n",
        "    # Show raw reranker scores\n",
        "    print(\"Top-k raw reranker scores:\", scores)\n",
        "\n",
        "    # Convert scores to 0-1 confidence using softmax\n",
        "    exp_scores = np.exp(scores - np.max(scores))\n",
        "    conf = float(np.max(exp_scores / exp_scores.sum()))\n",
        "    print(f\"Retrieved {len(chunks)} chunks. Confidence: {conf:.3f}\")\n",
        "\n",
        "    # Validate chunks\n",
        "    v_chunks, v_metas = validator_agent(chunks, metas)\n",
        "    if not v_chunks:\n",
        "        print('All chunks filtered by validator.')\n",
        "        return {'error':'all_filtered'}\n",
        "\n",
        "    # Generate MCQs: deterministic vs LLM\n",
        "    approach1 = deterministic_mcqs_from_chunks(v_chunks, n_q=5)\n",
        "    approach2 = None\n",
        "    if use_llm:\n",
        "        try:\n",
        "            approach2 = llm_generate_mcqs(v_chunks, n_q=5, api_key=api_key)\n",
        "        except Exception as e:\n",
        "            print('[WARN] LLM failed, falling back:', e)\n",
        "            approach2 = deterministic_mcqs_from_chunks(v_chunks, n_q=5)\n",
        "    else:\n",
        "        approach2 = deterministic_mcqs_from_chunks(v_chunks, n_q=5)\n",
        "\n",
        "    payload = finalizer_agent({'approach1': approach1, 'approach2': approach2}, v_metas)\n",
        "    return payload\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slSFH-cYxwhs"
      },
      "source": [
        "# Cell 14 — FastAPI Endpoint and Streamlit Demo Skeleton\n",
        "\n",
        "## Overview\n",
        "\n",
        "This cell sets up a **minimal web interface** and API endpoint** for the MESO system.  \n",
        "It enables external users (or a front-end) to **query the pipeline** and receive MCQs in real time.  \n",
        "Additionally, it provides a **Streamlit demo skeleton** for interactive testing.\n",
        "\n",
        "The main components are:\n",
        "\n",
        "1. **FastAPI endpoint** (`/generate`): Accepts a query, retrieves relevant chunks, validates, generates MCQs, and returns the payload.\n",
        "2. **Streamlit demo** (`streamlit_app.py`): Simple front-end that allows a user to input queries and view the results.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QEaO9jHAjMz",
        "outputId": "a323e5f0-9acd-494b-aba1-43c243b1238a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote streamlit_app.py — run with: uvicorn this_script:app --reload  and streamlit run streamlit_app.py\n"
          ]
        }
      ],
      "source": [
        "# CELL 14 — FastAPI endpoint and Streamlit demo skeleton\n",
        "# =====================\n",
        "# FastAPI:\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "class QueryIn(BaseModel):\n",
        "    query: str\n",
        "    top_k: int = 5\n",
        "    use_llm: bool = False\n",
        "\n",
        "@app.post('/generate')\n",
        "def generate(q: QueryIn):\n",
        "    idx, emb, meta = load_index_and_meta()\n",
        "    with open(CORPUS_PKL,'rb') as f:\n",
        "        d = pickle.load(f)\n",
        "        corpus = d['corpus']\n",
        "        meta = d['meta']\n",
        "    res = run_pipeline(q.query, index=idx, corpus=corpus, meta=meta, top_k=q.top_k, use_llm=q.use_llm, api_key=os.getenv('OPENAI_API_KEY'))\n",
        "    return res\n",
        "\n",
        "# Streamlit app (save as streamlit_app.py)\n",
        "streamlit_code = r\"\"\"\n",
        "import streamlit as st\n",
        "import requests\n",
        "st.title('EduRAG - Demo')\n",
        "u = st.text_input('Query', 'Photosynthesis')\n",
        "use_llm = st.checkbox('Use LLM for MCQs (requires API key on server)', False)\n",
        "if st.button('Generate'):\n",
        "    res = requests.post('http://localhost:8000/generate', json={'query':u, 'top_k':5, 'use_llm': use_llm})\n",
        "    st.json(res.json())\n",
        "\"\"\"\n",
        "with open('streamlit_app.py','w') as f:\n",
        "    f.write(streamlit_code)\n",
        "print('Wrote streamlit_app.py — run with: uvicorn this_script:app --reload  and streamlit run streamlit_app.py')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30mP3z8Px8Rm"
      },
      "source": [
        "# Utilities: Compare Approach 1 vs 2 and Save Report\n",
        "\n",
        "## Overview\n",
        "\n",
        "This cell provides a **utility function** to generate a human-readable **comparison report** between two MCQ generation approaches:\n",
        "\n",
        "1. **Approach 1** — Deterministic MCQs (TF-IDF / heuristic-based).\n",
        "2. **Approach 2** — LLM-based MCQs (e.g., OpenAI or Gemini).\n",
        "\n",
        "It saves the comparison as a **text file**, which can be shared, analyzed, or archived for educational evaluation.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9eq3TngAp2r"
      },
      "outputs": [],
      "source": [
        "# CELL 15 — Utilities: compare Approach 1 vs 2 and save report\n",
        "# =====================\n",
        "from pathlib import Path\n",
        "\n",
        "def compare_and_save_report(query: str, res: dict, filename: str = \"mcq_report.txt\"):\n",
        "    \"\"\"\n",
        "    Generate a text report comparing Approach1 (deterministic) and Approach2 (LLM),\n",
        "    and save to a file.\n",
        "    \"\"\"\n",
        "    report_lines = []\n",
        "    report_lines.append(f\"MCQ Generation Report for Query: '{query}'\\n\")\n",
        "    report_lines.append(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    # Approach 1 Section\n",
        "    report_lines.append(\"APPROACH 1: Deterministic MCQs\\n\")\n",
        "    report_lines.append(\"-\"*60 + \"\\n\")\n",
        "    approach1 = res.get('approach1', [])\n",
        "    if approach1:\n",
        "        for i, q in enumerate(approach1, 1):\n",
        "            report_lines.append(f\"{i}. {q}\\n\")\n",
        "    else:\n",
        "        report_lines.append(\"No MCQs generated.\\n\")\n",
        "\n",
        "    report_lines.append(\"\\n\")\n",
        "\n",
        "    # Approach 2 Section\n",
        "    report_lines.append(\"APPROACH 2: LLM-based MCQs\\n\")\n",
        "    report_lines.append(\"-\"*60 + \"\\n\")\n",
        "    approach2 = res.get('approach2', [])\n",
        "    if approach2:\n",
        "        for i, q in enumerate(approach2, 1):\n",
        "            report_lines.append(f\"{i}. {q}\\n\")\n",
        "    else:\n",
        "        report_lines.append(\"No MCQs generated.\\n\")\n",
        "\n",
        "    # Save to file\n",
        "    report_path = Path(filename)\n",
        "    with open(report_path, 'w', encoding='utf-8') as f:\n",
        "        f.writelines(report_lines)\n",
        "\n",
        "    print(f\"Report saved to {report_path.resolve()}\")\n",
        "    return report_path\n",
        "\n",
        "# Example usage\n",
        "# report_file = compare_and_save_report('Explain Economics', res)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7d2w7pryEg-"
      },
      "source": [
        "# Colab-only PDF Upload Helper\n",
        "\n",
        "## Overview\n",
        "\n",
        "This cell provides a **user-friendly helper** for Google Colab that allows users to **upload PDFs interactively**. It automatically saves the files to a designated folder (`outputs/uploads`) and provides a visual summary of the uploaded files. This helper is designed to streamline the **initial data ingestion step** in the workflow.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0qISbSPBtN1"
      },
      "outputs": [],
      "source": [
        "# =====================\n",
        "# CELL 3 — Colab-only PDF Upload Helper\n",
        "# =====================\n",
        "from google.colab import files\n",
        "from IPython.display import display, HTML\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "UPLOAD_DIR = Path('outputs/uploads')\n",
        "UPLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def colab_upload_handler_interactive():\n",
        "    print(\"📂 Click 'Choose Files' to select PDFs from your computer...\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        print(\"⚠️ No files uploaded.\")\n",
        "        return []\n",
        "\n",
        "    saved_paths = []\n",
        "    print(\"\\n💾 Saving files to\", UPLOAD_DIR, \"...\")\n",
        "    for name, blob in tqdm(uploaded.items(), desc=\"Uploading PDFs\"):\n",
        "        dest = UPLOAD_DIR / name\n",
        "        with open(dest, 'wb') as f:\n",
        "            f.write(blob)\n",
        "        saved_paths.append(str(dest))\n",
        "\n",
        "    # Interactive HTML summary\n",
        "    html_list = \"<ul>\"\n",
        "    for p in saved_paths:\n",
        "        html_list += f\"<li>{p}</li>\"\n",
        "    html_list += \"</ul>\"\n",
        "    display(HTML(f\"<b>✅ Successfully uploaded {len(saved_paths)} file(s):</b> {html_list}\"))\n",
        "\n",
        "    print(\"\\nNext steps:\")\n",
        "    print(\"1️⃣ Build corpus: corpus, meta = build_corpus_from_uploads('outputs/uploads')\")\n",
        "    print(\"2️⃣ Build FAISS index: index, emb = build_faiss_index(corpus, meta)\")\n",
        "    print(\"3️⃣ Run queries: run_pipeline('Your query here', index=index, corpus=corpus, meta=meta, top_k=5)\")\n",
        "\n",
        "    return saved_paths\n",
        "\n",
        "# Usage:\n",
        "# uploaded_files = colab_upload_handler_interactive()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3BfT4_0u3Jy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gsd5ZhuKs2Td"
      },
      "source": [
        "# Execution Flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284,
          "referenced_widgets": [
            "be0c0739e2c344ccae7f06ba0c063124",
            "4b48869a484541a99b6d09b38967efc5",
            "f2fb10254f4c4160a6d8b00d2c642793",
            "4bcb2fac147b4bf4833ef69fce29c2ac",
            "13f05b766b224fba94b3b770287db78f",
            "9463a47bb174459493bf5a27706d26b0",
            "e7de69441c514545b0bf9175e574e0db",
            "c51daa86596c4391a41c61b34b20caa4",
            "ddc8e4761fb54adfbfab4c690426bcd4",
            "c499ce9ca8b540ba87c475b8b4ffa700",
            "bb3ede0f900d470e8f99f58a229763c4"
          ]
        },
        "id": "xLPAA72HAv0X",
        "outputId": "1533b1b2-b57c-4d48-8981-d1236078566e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📂 Click 'Choose Files' to select PDFs from your computer...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e54ae51a-a873-44a5-a837-e074656e2eeb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e54ae51a-a873-44a5-a837-e074656e2eeb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kest101.pdf to kest101 (1).pdf\n",
            "\n",
            "💾 Saving files to outputs/uploads ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be0c0739e2c344ccae7f06ba0c063124",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Uploading PDFs:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<b>✅ Successfully uploaded 1 file(s):</b> <ul><li>outputs/uploads/kest101 (1).pdf</li></ul>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Next steps:\n",
            "1️⃣ Build corpus: corpus, meta = build_corpus_from_uploads('outputs/uploads')\n",
            "2️⃣ Build FAISS index: index, emb = build_faiss_index(corpus, meta)\n",
            "3️⃣ Run queries: run_pipeline('Your query here', index=index, corpus=corpus, meta=meta, top_k=5)\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Upload PDFs to process\n",
        "# Example (when everything is ready):\n",
        "uploaded = colab_upload_handler_interactive() # Returns list of saved PDF paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ko9uLNuhvaDV",
        "outputId": "e3c662ee-806e-473b-c07d-9a06a8cc5a31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVajoIcmvA7U",
        "outputId": "a812ed2d-593b-4a9e-a385-bddc00c18694"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ punkt_tab is now available\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# Standard tokenizer (already available)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tabular/paragraph-aware tokenizer (needed for PDF chunking)\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Verify availability\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab/english')\n",
        "    print(\"✅ punkt_tab is now available\")\n",
        "except LookupError:\n",
        "    print(\"❌ punkt_tab still not found. Try upgrading NLTK.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPjWlHG7tMt_",
        "outputId": "6556e839-d4b9-48ba-aa8b-e6c14e931c83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunks retrieved: 5\n",
            "Chunk 1: told this subject is mainly around what Alfred Marshall (one of the founders of modern economics) called “the study of man in the ordinary business of life”. Let us understand what that means. When yo ...\n",
            "\n",
            "Chunk 2: 5 be possible without data on various factors underlying an economic problem? And, that, in such a situation, no policies can be formulated to solve it. If yes, then you have, to a large extent, under ...\n",
            "\n",
            "Chunk 3: 4 may want to know how many are illiterate, who will not get jobs, requiring education, how many are highly educated and will have the best job opportunities and so on. In other words, you may want to ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Extract text, clean headers, chunk text into semantic chunks\n",
        "# This generates a corpus list and metadata for each chunk\n",
        "chunks, metas, scores = search_and_rerank(index, 'Understanding Economics', corpus, meta)\n",
        "print(\"Chunks retrieved:\", len(chunks))\n",
        "for i, c in enumerate(chunks[:3]):\n",
        "    print(f\"Chunk {i+1}:\", c[:200], \"...\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "470d02f6f77e4c8bb529cc5c27fb4cac",
            "463de1c089ee4fe5bb8866c8cca1c689",
            "ff59ebe8847844bba05affe60afa7a31",
            "6c65ecf1cbf8432093f010d7c6b2b2a3",
            "7d188648100647adae8dd50af9dc7ae4",
            "fd49e3de0cdf4b0a943bc55e8b32bd07",
            "99dc787c321a4371a5e9ba02ff924ca9",
            "6464adcfbb1a45b88edc7281f34b1f3d",
            "5902518694f6488e836ba79b794cc855",
            "51f428bbeda945d48d3822ee74b16041",
            "9656237ab4de45ed84e579107c5bfdd8"
          ]
        },
        "id": "v9jjuPxotQaX",
        "outputId": "c2c3a4d2-6e11-45d1-d355-5560e9bcb0a7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "470d02f6f77e4c8bb529cc5c27fb4cac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Embedding corpus:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved FAISS index to /content/outputs/faiss\n",
            "Embedding matrix shape: (8, 384)\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Convert corpus to embeddings and build FAISS index for semantic search\n",
        "index, emb = build_faiss_index(corpus, meta, encoder_model=None, batch_size=EMBED_BATCH, persist_dir=FAISS_DIR)\n",
        "\n",
        "# Verify embedding dimension\n",
        "print(\"Embedding matrix shape:\", emb.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2i3cLujRxOOO",
        "outputId": "309386ed-6558-46c5-ad30-6b21e0eaeaa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunks after guardrails: 4\n",
            "Validated Chunk 1: told this subject is mainly around what Alfred Marshall (one of the founders of modern economics) called “the study of man in the ordinary business of life”. Let us understand what that means. When yo ...\n",
            "\n",
            "Validated Chunk 2: 5 be possible without data on various factors underlying an economic problem? And, that, in such a situation, no policies can be formulated to solve it. If yes, then you have, to a large extent, under ...\n",
            "\n",
            "Validated Chunk 3: 4 may want to know how many are illiterate, who will not get jobs, requiring education, how many are highly educated and will have the best job opportunities and so on. In other words, you may want to ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 3: After validator\n",
        "v_chunks, v_metas = validator_agent(chunks, metas)\n",
        "print(\"Chunks after guardrails:\", len(v_chunks))\n",
        "for i, c in enumerate(v_chunks[:3]):\n",
        "    print(f\"Validated Chunk {i+1}:\", c[:200], \"...\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKUEid4LtTLS",
        "outputId": "236ca175-ee94-4c5c-ba1d-85779c52949a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-k raw reranker scores: [1.859816551208496, -2.0850226879119873, -2.245616912841797, -2.7816452980041504, -3.3252665996551514]\n",
            "Retrieved 5 chunks. Confidence: 0.951\n",
            "Keys in payload: dict_keys(['quiz', 'sources', 'generated_at'])\n",
            "Sample MCQ from deterministic approach:\n",
            "{'question': 'STATISTICS IN ECONOMICS In the previous section you were told about _____ special studies that concern the basic problems facing a country.', 'options': ['lines', 'finance', 'certain', 'formulation'], 'answer': 'certain', 'rationale': 'Taken from source sentence.', 'difficulty': 'medium'}\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Run a query to retrieve relevant chunks\n",
        "query = \"Understanding Economics\"\n",
        "\n",
        "# Run pipeline: retrieves chunks, validates, generates MCQs\n",
        "payload = run_pipeline(query=query, index=index, corpus=corpus, meta=meta, top_k=5, use_llm=False)\n",
        "\n",
        "# Check what we got\n",
        "print(\"Keys in payload:\", payload.keys())\n",
        "print(\"Sample MCQ from deterministic approach:\")\n",
        "print(payload['quiz']['approach1'][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJKLhHR-tZq9",
        "outputId": "bdf76447-756c-48d5-9a53-2ca184fc9dd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-k raw reranker scores: [1.859816551208496, -2.0850226879119873, -2.245616912841797, -2.7816452980041504, -3.3252665996551514]\n",
            "Retrieved 5 chunks. Confidence: 0.951\n",
            "[WARN] LLM failed, falling back: name 'llm_generate_mcqs' is not defined\n",
            "Sample MCQ from LLM approach:\n",
            "{'question': 'The chief characteristic of such information is that they describe _____ of a single person or a group of persons that is important to record as accurately as possible even though they cannot be measured in quantitative terms.', 'options': ['enormously', 'concerned', 'resources', 'attributes'], 'answer': 'attributes', 'rationale': 'Taken from source sentence.', 'difficulty': 'medium'}\n"
          ]
        }
      ],
      "source": [
        "# Step 5: If you have an LLM API key, enable LLM MCQ generation\n",
        "# Make sure you have OPENAI_API_KEY or GEMINI_API_KEY set in environment\n",
        "payload_llm = run_pipeline(query=query, index=index, corpus=corpus, meta=meta, top_k=5, use_llm=True, api_key=os.getenv('OPENAI_API_KEY'))\n",
        "\n",
        "print(\"Sample MCQ from LLM approach:\")\n",
        "print(payload_llm['quiz']['approach2'][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXILb11OteOe",
        "outputId": "176b3ca6-130e-46c5-d994-3c8fcabbc43e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunks before validator: 5, after validator: 4\n"
          ]
        }
      ],
      "source": [
        "# Step 6: You can test the validator on retrieved chunks\n",
        "chunks, metas, scores = search_and_rerank(index, query, corpus, meta, top_k=5)\n",
        "v_chunks, v_metas = validator_agent(chunks, metas)\n",
        "print(f\"Chunks before validator: {len(chunks)}, after validator: {len(v_chunks)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtETmxiTth0h",
        "outputId": "00c243f2-7932-4de6-8c9a-18c01e1ccec7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q1: Statistics also helps in condensing mass data into a few numerical measures (such as mean, variance etc., about which you will _____ later).\n",
            "Options: ['learn', 'overall', 'courses', 'marshall'], Answer: learn\n",
            "\n",
            "Q2: At this stage you are probably _____ to know more about Statistics.\n",
            "Options: ['variance', 'knowing', 'ready', 'distinguishes'], Answer: ready\n",
            "\n",
            "Q3: Would you now agree with the following _____ of economics that many economists use?\n",
            "Options: ['goods', 'information', 'increased', 'definition'], Answer: definition\n",
            "\n",
            "Q4: An economist may be interested in finding out what _____ to the demand for a commodity when its price increases or decreases?\n",
            "Options: ['price', 'physics', 'happens', 'statement'], Answer: happens\n",
            "\n",
            "Q5: The data, then, are summarised by calculating various numerical indices, such as mean, _____, standard deviation, etc., that represent the broad characteristics of the collected set of information.\n",
            "Options: ['variance', 'includes', 'distribute', 'business'], Answer: variance\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 7: Generate MCQs from validated chunks (deterministic approach)\n",
        "mcqs = deterministic_mcqs_from_chunks(v_chunks, n_q=5)\n",
        "for i, q in enumerate(mcqs, 1):\n",
        "    print(f\"Q{i}: {q['question']}\")\n",
        "    print(f\"Options: {q['options']}, Answer: {q['answer']}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNncrMeqtkZk",
        "outputId": "57ac6565-fd1b-4ac9-fb9a-f1c7fb8636ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No valid images found in outputs/uploads\n",
            "Multimodal image indexing done: None\n"
          ]
        }
      ],
      "source": [
        "# Step 8: If you want to index images alongside PDFs (optional)\n",
        "image_indexed = multimodal_index_images(image_folder='outputs/uploads', persist_dir=CLIP_DIR)\n",
        "print(\"Multimodal image indexing done:\", image_indexed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGBNAR9ZtnFA",
        "outputId": "15d0e8eb-ea9a-4398-c968-a4fc10dbd35a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Worksheet saved at: mcq_worksheet3.txt\n"
          ]
        }
      ],
      "source": [
        "# Step 9: Compare deterministic vs LLM MCQs for the same query\n",
        "# Prepare payload in the format expected by the report function\n",
        "report_payload = {\n",
        "    \"approach1\": mcqs,          # deterministic MCQs\n",
        "    \"approach2\": payload_llm     # LLM MCQs (may be empty)\n",
        "}\n",
        "\n",
        "# Check if LLM MCQs are empty\n",
        "llm_empty = not payload_llm\n",
        "if llm_empty:\n",
        "    print(\"⚠️ Warning: LLM MCQs are empty. Using deterministic MCQs only.\")\n",
        "\n",
        "# Function to create nicely formatted MCQ worksheet\n",
        "def create_mcq_worksheet(query, deterministic_mcqs, llm_mcqs, filename=\"mcq_worksheet.txt\"):\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"MCQ Generation Worksheet for Query: '{query}'\\n\")\n",
        "        f.write(\"=\"*80 + \"\\n\\n\")\n",
        "\n",
        "        # Deterministic MCQs\n",
        "        f.write(\"APPROACH 1: Deterministic MCQs\\n\")\n",
        "        f.write(\"-\"*80 + \"\\n\")\n",
        "        for i, q in enumerate(deterministic_mcqs, 1):\n",
        "            f.write(f\"{i}. {q.get('question','N/A')}\\n\")\n",
        "            for j, opt in enumerate(q.get('options', []), 1):\n",
        "                f.write(f\"   {chr(64+j)}. {opt}\\n\")\n",
        "            f.write(f\"Answer: {q.get('answer','N/A')}\\n\")\n",
        "            f.write(f\"Rationale: {q.get('rationale', 'N/A')}\\n\\n\")\n",
        "\n",
        "        # LLM MCQs\n",
        "        f.write(\"APPROACH 2: LLM-based MCQs\\n\")\n",
        "        f.write(\"-\"*80 + \"\\n\")\n",
        "\n",
        "        # Check if LLM MCQs are valid list of dicts\n",
        "        if not llm_mcqs or not isinstance(llm_mcqs, list) or not all(isinstance(q, dict) for q in llm_mcqs):\n",
        "            f.write(\"⚠️ No valid LLM-based MCQs were generated for this query.\\n\")\n",
        "        else:\n",
        "            for i, q in enumerate(llm_mcqs, 1):\n",
        "                f.write(f\"{i}. {q.get('question','N/A')}\\n\")\n",
        "                for j, opt in enumerate(q.get('options', []), 1):\n",
        "                    f.write(f\"   {chr(64+j)}. {opt}\\n\")\n",
        "                f.write(f\"Answer: {q.get('answer','N/A')}\\n\")\n",
        "                f.write(f\"Rationale: {q.get('rationale', 'N/A')}\\n\\n\")\n",
        "\n",
        "    return filename\n",
        "\n",
        "# Save worksheet\n",
        "worksheet_file = create_mcq_worksheet(query=query,\n",
        "                                      deterministic_mcqs=mcqs,\n",
        "                                      llm_mcqs=payload_llm,\n",
        "                                      filename=\"mcq_worksheet3.txt\")\n",
        "\n",
        "print(\"Worksheet saved at:\", worksheet_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZJpikRmtpU3",
        "outputId": "cf95ab06-9b21-46b8-c2c5-985b38a6c925"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded FAISS index and meta. n_chunks= 8\n",
            "Number of chunks in loaded index: 8\n"
          ]
        }
      ],
      "source": [
        "# Step 10: Load persisted FAISS index and embeddings to confirm\n",
        "index_loaded, emb_loaded, meta_loaded = load_index_and_meta(persist_dir=FAISS_DIR)\n",
        "print(\"Number of chunks in loaded index:\", len(meta_loaded))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLYBRIlftr5w"
      },
      "outputs": [],
      "source": [
        "# Step 11: If you want to run the interactive demo\n",
        "# FastAPI endpoint is already created in your code\n",
        "# Run this in terminal:\n",
        "# uvicorn this_script:app --reload\n",
        "# Then run streamlit:\n",
        "# streamlit run streamlit_app.py\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "13f05b766b224fba94b3b770287db78f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "463de1c089ee4fe5bb8866c8cca1c689": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd49e3de0cdf4b0a943bc55e8b32bd07",
            "placeholder": "​",
            "style": "IPY_MODEL_99dc787c321a4371a5e9ba02ff924ca9",
            "value": "Embedding corpus: 100%"
          }
        },
        "470d02f6f77e4c8bb529cc5c27fb4cac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_463de1c089ee4fe5bb8866c8cca1c689",
              "IPY_MODEL_ff59ebe8847844bba05affe60afa7a31",
              "IPY_MODEL_6c65ecf1cbf8432093f010d7c6b2b2a3"
            ],
            "layout": "IPY_MODEL_7d188648100647adae8dd50af9dc7ae4"
          }
        },
        "4b48869a484541a99b6d09b38967efc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9463a47bb174459493bf5a27706d26b0",
            "placeholder": "​",
            "style": "IPY_MODEL_e7de69441c514545b0bf9175e574e0db",
            "value": "Uploading PDFs: 100%"
          }
        },
        "4bcb2fac147b4bf4833ef69fce29c2ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c499ce9ca8b540ba87c475b8b4ffa700",
            "placeholder": "​",
            "style": "IPY_MODEL_bb3ede0f900d470e8f99f58a229763c4",
            "value": " 1/1 [00:00&lt;00:00, 90.76it/s]"
          }
        },
        "51f428bbeda945d48d3822ee74b16041": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5902518694f6488e836ba79b794cc855": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6464adcfbb1a45b88edc7281f34b1f3d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c65ecf1cbf8432093f010d7c6b2b2a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51f428bbeda945d48d3822ee74b16041",
            "placeholder": "​",
            "style": "IPY_MODEL_9656237ab4de45ed84e579107c5bfdd8",
            "value": " 1/1 [00:02&lt;00:00,  2.52s/it]"
          }
        },
        "7d188648100647adae8dd50af9dc7ae4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9463a47bb174459493bf5a27706d26b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9656237ab4de45ed84e579107c5bfdd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99dc787c321a4371a5e9ba02ff924ca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb3ede0f900d470e8f99f58a229763c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be0c0739e2c344ccae7f06ba0c063124": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b48869a484541a99b6d09b38967efc5",
              "IPY_MODEL_f2fb10254f4c4160a6d8b00d2c642793",
              "IPY_MODEL_4bcb2fac147b4bf4833ef69fce29c2ac"
            ],
            "layout": "IPY_MODEL_13f05b766b224fba94b3b770287db78f"
          }
        },
        "c499ce9ca8b540ba87c475b8b4ffa700": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c51daa86596c4391a41c61b34b20caa4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddc8e4761fb54adfbfab4c690426bcd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e7de69441c514545b0bf9175e574e0db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2fb10254f4c4160a6d8b00d2c642793": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c51daa86596c4391a41c61b34b20caa4",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ddc8e4761fb54adfbfab4c690426bcd4",
            "value": 1
          }
        },
        "fd49e3de0cdf4b0a943bc55e8b32bd07": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff59ebe8847844bba05affe60afa7a31": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6464adcfbb1a45b88edc7281f34b1f3d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5902518694f6488e836ba79b794cc855",
            "value": 1
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
